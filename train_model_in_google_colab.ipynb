{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"cr_EpqcIS_dm"},"source":["# Import lib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iL_iNwkmS_dn"},"outputs":[],"source":["import glob as glob\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","\n","np.random.seed(42)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eOmZbFC7okY6"},"source":["# Google Colab mount drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGan2OsIT65H"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8w6V_udLS_dv"},"source":["# Plot annotated images"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b5JJexRel6Xd"},"source":["Code below is copy with my small changes from official YOLO tutorials"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q9Bu6ua2S_dv"},"outputs":[],"source":["class_names = [\"bath\",\n","\"dishwasher / washing machine\",\n","\"door\",\n","\"furniture\",\n","\"refrigerator\",\n","\"sink\",\n","\"stairs\",\n","\"stove\",\n","\"table\",\n","\"wall\",\n","\"wc\",\n","\"window\"]\n","colors = np.random.uniform(0, 255, size=(len(class_names), 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8yZmTRTS_dw"},"outputs":[],"source":["# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.\n","def yolo2bbox(bboxes):\n","    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n","    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n","    return xmin, ymin, xmax, ymax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHODw5_TS_dw"},"outputs":[],"source":["def plot_box(image, bboxes, labels):\n","    # Need the image height and width to denormalize\n","    # the bounding box coordinates\n","    h, w, _ = image.shape\n","    for box_num, box in enumerate(bboxes):\n","        x1, y1, x2, y2 = yolo2bbox(box)\n","        # denormalize the coordinates\n","        xmin = int(x1*w)\n","        ymin = int(y1*h)\n","        xmax = int(x2*w)\n","        ymax = int(y2*h)\n","        width = xmax - xmin\n","        height = ymax - ymin\n","\n","        class_name = class_names[int(labels[box_num])]\n","\n","        cv2.rectangle(\n","            image,\n","            (xmin, ymin), (xmax, ymax),\n","            color=colors[class_names.index(class_name)],\n","            thickness=1\n","        )\n","\n","        font_scale = min(1,max(3,int(w/500)))\n","        font_thickness = min(2, max(10,int(w/50)))\n","\n","        p1, p2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n","        # Text width and height\n","        tw, th = cv2.getTextSize(\n","            class_name,\n","            0, fontScale=font_scale, thickness=font_thickness\n","        )[0]\n","        p2 = p1[0] + tw, p1[1] + -th - 10\n","        cv2.rectangle(\n","            image,\n","            p1, p2,\n","            color=colors[class_names.index(class_name)],\n","            thickness=-1,\n","        )\n","        cv2.putText(\n","            image,\n","            class_name,\n","            (xmin+1, ymin-12),\n","            cv2.FONT_HERSHEY_SIMPLEX,\n","            font_scale,\n","            (255, 255, 255),\n","            font_thickness\n","        )\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WC7sGp2mS_dx"},"outputs":[],"source":["# Function to plot images with the bounding boxes.\n","def plot(image_paths, label_paths, num_samples):\n","    all_training_images = glob.glob(image_paths)\n","    all_training_labels = glob.glob(label_paths)\n","    all_training_images.sort()\n","    all_training_labels.sort()\n","\n","    num_images = len(all_training_images)\n","\n","    plt.figure(figsize=(25, 20))\n","    for i in range(num_samples):\n","        j = random.randint(0,num_images-1)\n","        image = cv2.imread(all_training_images[j])\n","        with open(all_training_labels[j], 'r') as f:\n","            bboxes = []\n","            labels = []\n","            label_lines = f.readlines()\n","            for label_line in label_lines:\n","                # label = label_line[0]\n","                # bbox_string = label_line[2:]\n","                # print(bbox_string)\n","                num, x_c, y_c, w, h = label_line.split(' ')\n","                x_c = float(x_c)\n","                y_c = float(y_c)\n","                w = float(w)\n","                h = float(h)\n","                bboxes.append([x_c, y_c, w, h])\n","                # print(bboxes)\n","                labels.append(num)\n","        result_image = plot_box(image, bboxes, labels)\n","        # print(result_image)\n","        plt.subplot(2, 2, i+1)\n","        plt.imshow(result_image[:, :, ::-1])\n","        plt.axis('off')\n","    plt.subplots_adjust(wspace=0)\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeD6lNGwVWQS"},"outputs":[],"source":["# Visualize a few training images. For google collab\n","PATH = \"/content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset\"\n","plot(\n","    image_paths=PATH + '/images/training/*',\n","    label_paths=PATH + '/labels/training/*',\n","    num_samples=1,\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"i1nxgcQKr5Hu"},"source":["# Set help function for dashboard"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Hiibbp_-l-Jd"},"source":["Code below is copy with my small changes from official YOLO tutorials"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5x9FFm3mV0PU"},"source":["Helper Functions for Logging\n","\n","Here, we write the helper functions that we need for logging of the results in the notebook while training the models.\n","\n","Let's create our custom result directories so that we can easily keep track of them and carry out inference using the proper model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a66ba3a9"},"outputs":[],"source":["def set_res_dir():\n","    # Directory to store results\n","    res_dir_count = len(glob.glob('runs/train/*'))\n","    print(f\"Current number of result directories: {res_dir_count}\")\n","    if TRAIN:\n","        RES_DIR = f\"results_{res_dir_count+1}\"\n","        print(RES_DIR)\n","    else:\n","        RES_DIR = f\"results_{res_dir_count}\"\n","    return RES_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2F8v9XmChexr"},"outputs":[],"source":["pip install -U tensorboard-plugin-profile\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uTJi_GiwV26X"},"source":["Function to Monitor TensorBoard logs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pHBNjo7F2QM7"},"outputs":[],"source":["%reload_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fanu3LXXV44i"},"outputs":[],"source":["def monitor_tensorboard():\n","    %load_ext tensorboard\n","    %tensorboard --logdir runs/train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eA1vjRtEWN1z"},"outputs":[],"source":["monitor_tensorboard()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kKm1casothJ_"},"source":["# Clone Yolov5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gmk4pUqdV_U2"},"outputs":[],"source":["# Clono Yolov5 repo\n","if not os.path.exists('yolov5'):\n","    !git clone https://github.com/ultralytics/yolov5.git"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"DDaze6OKWB2u"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/pan_pawel/Documents/GitHub/AECVision/yolov5\n","/home/pan_pawel/Documents/GitHub/AECVision/yolov5\n"]}],"source":["%cd yolov5/\n","!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6unvIcRWCYU"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_DCXJ1nmWMAb"},"source":["# Train model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-zOc3ldfWwJU"},"source":["\n","Hyperparameters and Constants\n","\n","Here, we define wether to train the model or not and for how many epochs to train for.\n","\n","If TRAIN = False, then the last trained model will be used for inference in the notebook if run end to end."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sKmGQ0KWueX"},"outputs":[],"source":["TRAIN = True\n","# Number of epochs to train. Default in my learning process is 300\n","EPOCHS = 300"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"UDVKIbO-xH6t"},"source":["## Train on pretrained weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXgIaY2OWTgN"},"outputs":[],"source":["RES_DIR = set_res_dir()\n","if TRAIN:\n","    !python train.py --data /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/dataset_googlecollab.yaml --weights yolov5m.pt \\\n","    --img 1280 --epochs {EPOCHS} --batch-size 8 --name {RES_DIR} --cache --project /content/drive/MyDrive/ColabNotebooks/AECVision/train_results --name  traine_"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"N-U0n3E-ucsx"},"source":["## Train by use set hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q85DnETWyCwT"},"outputs":[],"source":["RES_DIR = set_res_dir()\n","if TRAIN:\n","    !python train.py --data /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/dataset_googlecollab.yaml --weights yolov5s6.pt \\\n","    --img 1280 --epochs {EPOCHS} --batch-size 20 --name {RES_DIR} --cache --hyp /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/hyp_my.yaml --project /content/drive/MyDrive/ColabNotebooks/AECVision/train_results --name  traine_"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xwx9rKJ0vxLV"},"source":["## Train without pretrained weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYv5hqPdvsWq"},"outputs":[],"source":["RES_DIR = set_res_dir()\n","if TRAIN:\n","    !python train.py --data /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/dataset_googlecollab.yaml --weights \"\" --cfg yolov5s6.yaml \\\n","    --img 1280 --epochs {EPOCHS} --batch-size 20 --name {RES_DIR} --hyp /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/hyp.no-augmentation.yaml --project /content/drive/MyDrive/ColabNotebooks/AECVision/train_results --name  traine_"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rMUuBaGnksPV"},"source":["## Test on validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5T-rGLKkznk"},"outputs":[],"source":["if TRAIN:\n","    !python val.py --data /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/dataset_googlecollab.yaml --weights /content/drive/MyDrive/ColabNotebooks/AECVision/train_results/traine_best/weights/best.pt \\\n","    --img 1280 --name {RES_DIR} --project /content/drive/MyDrive/ColabNotebooks/AECVision/train_results --name  validation_"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZxAr2uwqukOq"},"source":["## Search for best hyperparameters"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zxFBwClpwOFE"},"source":["Weights are from yolo5s because of ram use (bigger model affects ram). Batch size is default 16 maybe could be bigger."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8UlU8gIhpDM"},"outputs":[],"source":["RES_DIR = set_res_dir()\n","if TRAIN:\n","    !python train.py --data /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/dataset_googlecollab.yaml --weights yolov5s6.pt \\\n","    --img 1280 --epochs {EPOCHS} --batch-size 16 --name {RES_DIR} --cache --evolve 100"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"g5O5BmchygAL"},"source":["### Plot results and save"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3m2Hr7yHToiq"},"outputs":[],"source":["# Utils is in Yolo requirements\n","from utils.plots import  plot_evolve\n","\n","plot_evolve(evolve_csv=\"/content/yolov5/runs/evolve/results_1/evolve.csv\" ) # You must change path to your evolve results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nFrtU6-hgqm"},"outputs":[],"source":["# Save evolve results\n","# Zip hyperparameters\n","!zip -r /content/drive/MyDrive/ColabNotebooks/AECVision/TrainRes/evolve/evolve.zip /content/yolov5/runs/evolve/\n","# Download\n","from google.colab import files\n","# files.download(\"/content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/images/prediction/prediction.zip\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AxoJ4INVxEkj"},"source":["# Prediction"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MIWu83qCzP8d"},"source":["## Detect images from dource folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tj8eUAFzxHLq"},"outputs":[],"source":["# conf - prediction confidence from 0 to 1\n","# source - folder with images\n","!python detect.py --weights /content/drive/MyDrive/ColabNotebooks/AECVision/train_results/traine_best/weights/best.pt --img 1280 --conf 0.8 --source /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/images/prediction --project /content/drive/MyDrive/ColabNotebooks/AECVision/detection --save-txt"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3ldCrgop47AK"},"source":["## Download predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaxUZLSs4-Ad"},"outputs":[],"source":["# Zip images\n","!zip -r /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/images/preannotations/prediction.zip /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/images/preannotations\n","# Zip labels\n","!zip -r /content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/labels/prediction/prediction_txt.zip /content/yolov5/runs/detect/exp/labels/\n","# Download\n","from google.colab import files\n","files.download(\"/content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/images/prediction/prediction.zip\")\n","files.download(\"/content/drive/MyDrive/ColabNotebooks/AECVision/small_dataset/labels/prediction/prediction_txt.zip\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/home/pan_pawel/Documents/GitHub/AECVision/train_results/traine_best/weights/best.pt'], source=0, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.8, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","\u001b[31m\u001b[1mrequirements:\u001b[0m /home/pan_pawel/Documents/GitHub/AECVision/requirements.txt not found, check failed.\n","YOLOv5 🚀 v7.0-147-gaa7c45c Python-3.9.16 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce GTX 1050, 4039MiB)\n","\n","Fusing layers... \n","YOLOv5s6 summary: 206 layers, 12350572 parameters, 0 gradients, 16.2 GFLOPs\n","WARNING ⚠️ Environment does not support cv2.imshow() or PIL Image.show()\n","OpenCV(4.7.0) /io/opencv/modules/highgui/src/window.cpp:1272: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n","\n","1/1: 0...  Success (inf frames 640x480 at 30.00 FPS)\n","\n","Traceback (most recent call last):\n","  File \"/home/pan_pawel/Documents/GitHub/AECVision/yolov5/detect.py\", line 261, in <module>\n","    main(opt)\n","  File \"/home/pan_pawel/Documents/GitHub/AECVision/yolov5/detect.py\", line 256, in main\n","    run(**vars(opt))\n","  File \"/home/pan_pawel/Documents/GitHub/AECVision/aec-env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/home/pan_pawel/Documents/GitHub/AECVision/yolov5/detect.py\", line 117, in run\n","    for path, im, im0s, vid_cap, s in dataset:\n","  File \"/home/pan_pawel/Documents/GitHub/AECVision/yolov5/utils/dataloaders.py\", line 410, in __next__\n","    if not all(x.is_alive() for x in self.threads) or cv2.waitKey(1) == ord('q'):  # q to quit\n","cv2.error: OpenCV(4.7.0) /io/opencv/modules/highgui/src/window.cpp:1338: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvWaitKey'\n","\n","terminate called without an active exception\n"]}],"source":["!python detect.py --weights /home/pan_pawel/Documents/GitHub/AECVision/train_results/traine_best/weights/best.pt --img 1280 --conf 0.8 --source 0"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","private_outputs":true,"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"aec-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
